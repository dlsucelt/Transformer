{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Pretraining\n",
    "By Jan Christian Blaise B. Cruz\n",
    "\n",
    "In this notebook we'll see how to pretrain a Transformer (Vaswani, et al., 2015) language model for the purposes of transfer learning it into a downstream task. First, let's do some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from models import TransformerForLanguageModeling\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of demonstration, we'll use the WikiText-2 language modeling dataset. This has 2M tokens in entirety. In real life, language models for transfer learning are often trained with the larger WikiText-103 (with 103M tokens) + a variety of other large corpora. We won't do this here as it is obviously resource-intensive. Training a Transformer on WikiText-103 along takes a couple days on an NVIDIA Tesla V100 to achieve robust results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wikitext2_train.txt', 'r') as f:\n",
    "    train_text = [l.strip().replace('<unk>', '[UNK]') for l in f]\n",
    "with open('data/wikitext2_valid.txt', 'r') as f:\n",
    "    valid_text = [l.strip().replace('<unk>', '[UNK]') for l in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tokenization, we'll use a scheme called \"WordPiece.\" WordPiece is a form of subword tokenization that uses the Byte-Pair Encoding (Seinnrich, et al., 2016) to chunk words into smaller \"pieces of words\" by using a compression algorithm and n-gram frequencies (more recent iterations use a language model to optimize the chunks that get into the final vocab list). This has two advantages: first, it allows us to represent words that are out of the vocabulary as all small chunks (single letters included) can form infinite combinations of tokens. Second, this scheme is actually morphologically closer to modeling language than word-based tokenization as we can now embed \"sounds\" or \"morphemes\" directly instead of individual tokens.\n",
    "\n",
    "I won't go into detail about how BPE works (the paper is well written). To save us the hassle of training a WordPiece vocabulary, we'll use the one trained with BERT (Devlin, et al., 2018), another Transformer-based language model. \n",
    "\n",
    "\n",
    "*Note: Do note that if you want to use another language other than English, you'd have to train your own WordPiece vocabulary (usually with a BERT model). We happen to have Filipino ones available (BERT models included) so let us know if you need them.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll prepare the dataset for training.\n",
    "\n",
    "Note that transformers, unlike Recurrent Neural Network (RNN) based models, can only take in sequences of fixed length. This is actually an advantage, that it can be fed inputs by batch and not sequentially (which limits the RNNs ability to be parallelized, crippling its speed). We set a \"maximum number of positions\" (synonymous to BPTT length) which is the same number of positions the transformer can attent to at one time using Multihead Attention.\n",
    "\n",
    "We'll write a function to do the processing for us. We truncate long sequences to the maximum positions minus one in order to add the special \"end of sequence\" token (BERT uses ```[CLS]``` and so we'll use the same). We also pad the shorter sequences. Since we're doing language modeling, we'll train the model to predict the next word, so we shift the tokens one position to the left to produce our target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(text, max_num_pos):\n",
    "    X_set, y_set = [], []\n",
    "    for line in tqdm(text):\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        x = tokens[:-1][:max_num_pos - 1] + ['[CLS]']\n",
    "        y = tokens[1:][:max_num_pos - 1] + ['[CLS]']\n",
    "\n",
    "        if len(x) < max_num_pos:\n",
    "            x = x + ['[PAD]' for _ in range(max_num_pos - len(x))]\n",
    "        if len(y) < max_num_pos:\n",
    "            y = y + ['[PAD]' for _ in range(max_num_pos - len(y))]\n",
    "\n",
    "        x = tokenizer.convert_tokens_to_ids(x)\n",
    "        y = tokenizer.convert_tokens_to_ids(y)\n",
    "\n",
    "        X_set.append(x)\n",
    "        y_set.append(y)\n",
    "    X_set = torch.LongTensor(X_set)\n",
    "    y_set = torch.LongTensor(y_set)\n",
    "    data = TensorDataset(X_set, y_set)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a maximum number of positions of 256 (the standard configuration for the Transformer-XL (Dai, wt al., 2019) model) and a batch size of 32 (to make sure it fits in a sizeable enough GPU). We construct the dataloaders to faciliate better batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36718/36718 [00:27<00:00, 1351.82it/s]\n",
      "100%|██████████| 3760/3760 [00:02<00:00, 1407.03it/s]\n"
     ]
    }
   ],
   "source": [
    "max_num_pos = 256\n",
    "batch_size = 32\n",
    "\n",
    "train_data = prepare_dataset(train_text, max_num_pos)\n",
    "valid_data = prepare_dataset(valid_text, max_num_pos)\n",
    "train_loader = DataLoader(train_data, batch_size)\n",
    "valid_loader = DataLoader(valid_data, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a transformer with a language modeling head on top for our task. The hyperparameters we use, although we are using the GPT-2 (Radford, et al., 2019) architecture, are akin to the settings used by the Transformer-XL. We'll use 10 heads to attend to 10 positions at once, through 16 layers of transformer blocks. Again, I won't go into detail with how Transformer work, but the aforementioned papers are good resources for learning about modern iterations of them.\n",
    "\n",
    "We'll train the model using Adam (Kingma & Ba, 2014) and use a standard cross entropy objective. We'll ignore the padding token while computing the loss. We'll also use cosine annealing to steadily decrease our learning rate. We initialize the weights of the network to a mean of 0 and a standard deviation of 0.02, akin to the settings of the Transformer-XL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 50,396,360 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = TransformerForLanguageModeling(embed_dim=410, hidden_dim=2100, num_embeddings=len(tokenizer.vocab), \n",
    "                                       num_max_positions=256, num_heads=10, num_layers=16, dropout=0.1).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.vocab['[PAD]'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=2.5e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, 10)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n",
    "        m.weight.data.normal_(mean=0.0, std=0.02)\n",
    "    if isinstance(m, (nn.Linear, nn.LayerNorm)) and m.bias is not None:\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(\"The model has {:,} trainable parameters\".format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only train for 10 epochs just for demonstration. Normally, you'd want to train for 200 epochs with a much larger dataset (like WikiText-103)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "max_norm = 0.25\n",
    "train_loss = 0\n",
    "train_ppl = 0\n",
    "test_loss = 0\n",
    "test_ppl = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        out = model(x)\n",
    "        loss = criterion(out.flatten(0, 1), y.flatten())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_ppl += torch.exp(loss).item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_ppl /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            out = model(x)\n",
    "            loss = criterion(out.flatten(0, 1), y.flatten())\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            test_ppl += torch.exp(loss).item()\n",
    "        test_loss /= len(valid_loader)\n",
    "        test_ppl /= len(valid_loader)\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\"Train Loss {:.4f} | Train Ppl {:.4f} | Test Loss {:.4f} | Test Ppl {:.4f}\".format(train_loss, train_ppl, test_loss, test_ppl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can then use the pretrained language model to finetune to a downstream task. In the next notebook, we'll use a pretrained transformer to perform text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
