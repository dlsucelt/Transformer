{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Finetuning\n",
    "By Jan Christian Blaise B. Cruz\n",
    "\n",
    "In this notebook, we'll learn how to finetune a transformer language model into a text classifier. First, let's do the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CyclicLR, CosineAnnealingLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, cached_path\n",
    "from models import TransformerForClassification\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finetune to either the IMDB Sentiment Classification task, or a subset of the Text Retrieval Conference (TREC) document classification task. For this example, we'll use the TREC dataset. The datasets are available directly in the ```/data``` folder so there's no need to download them separately. Like in the pretraining notebook, we'll use WordPiece tokenization and we'll use BERT's ready tokenizer for this purpose. \n",
    "\n",
    "Again, do note that if you'll want to use a different language, you'll have to train your own BERT WordPiece tokenizer (we have Filipino ones available, contact us if you need them!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "task = 'trec'\n",
    "df = pd.read_csv('data/' + task + '.csv')\n",
    "text, labels = list(df['text']), list(df['labels'])\n",
    "\n",
    "# Instantiate tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll preprocess the data like we did in the pretraining notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5452/5452 [00:00<00:00, 6318.37it/s]\n"
     ]
    }
   ],
   "source": [
    "max_num_pos = 256\n",
    "batch_size = 32\n",
    "\n",
    "# Trim the dataset and pad\n",
    "data = []\n",
    "for line in tqdm(text):\n",
    "    line = tokenizer.tokenize(line)[:max_num_pos - 1] + ['[CLS]']\n",
    "    if len(line) < max_num_pos:\n",
    "        line = line + ['[PAD]' for _ in range(max_num_pos - len(line))]\n",
    "    tokens = tokenizer.convert_tokens_to_ids(line)\n",
    "    data.append(tokens)\n",
    "X = np.array(data)\n",
    "\n",
    "# Build labels\n",
    "label_list = list(set(labels))\n",
    "y = np.array([label_list.index(y) for y in labels])\n",
    "\n",
    "# Build dataset and loader\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "train_data = TensorDataset(torch.LongTensor(X_train), torch.LongTensor(y_train))\n",
    "test_data = TensorDataset(torch.LongTensor(X_test), torch.LongTensor(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finetune, we'll use a GPT-2 Transformer model with a classification head on top (see the ```models.py``` file for the complete code listing). We'll again use the Adam (Kingma & Ba, 2014) optimizer to finetune our model, and we'll use Cosine Annealing as out learning rate schedule. We'll give it a maximum epoch setting of 3 since we'll only take that long to finetune our classifier.\n",
    "\n",
    "Likewise, we'll initialize the weights and biases. Afterwhich, we'll load pretrained Transformer weights trained on the WikiText-103 dataset. The script downloads it for you automatically, so there's no need to process it yourself. A nice thing to note here is that ```TransformerForClassification``` is a subclass of ```TransformerForLanguageModeling``` and so loading weights for the superclass works for the subclass. No need to adjust weights and encodings unlike in ULMFiT (Howard & Ruder, 2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 50,398,826 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = TransformerForClassification(num_classes=len(label_list), embed_dim=410, hidden_dim=2100, num_embeddings=len(tokenizer.vocab), \n",
    "                                     num_max_positions=256, num_heads=10, num_layers=16, dropout=0.1).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=6e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, 3)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n",
    "        m.weight.data.normal_(mean=0.0, std=0.02)\n",
    "    if isinstance(m, (nn.Linear, nn.LayerNorm)) and m.bias is not None:\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(\"The model has {:,} trainable parameters\".format(count_parameters(model)))\n",
    "\n",
    "# Download weights and load them\n",
    "path = cached_path('https://s3.amazonaws.com/models.huggingface.co/naacl-2019-tutorial/model_checkpoint.pth')\n",
    "state_dict = torch.load(open(path, 'rb'), map_location='cpu')\n",
    "incompatible_keys = model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll finetune for just three epochs. We'll use a max norm of 0.25 for gradient clipping.\n",
    "\n",
    "Do note that since the base model is a language model, our classifier will likewise anticipate an input of shape ```[seq_len, batch_size]```. We'll transpose accordingly to accomodate the need. Also, we're going to have to manually mask the special tokens ```[CLS]``` and ```[PAD]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:39<00:00,  3.51it/s]\n",
      "100%|██████████| 43/43 [00:04<00:00, 10.71it/s]\n",
      "  0%|          | 0/128 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.8024 | Train Acc 0.6962 | Test Loss 0.3232 | Test Acc 0.8912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:38<00:00,  3.54it/s]\n",
      "100%|██████████| 43/43 [00:04<00:00, 10.72it/s]\n",
      "  0%|          | 0/128 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.2756 | Train Acc 0.9143 | Test Loss 0.2851 | Test Acc 0.9376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:38<00:00,  3.50it/s]\n",
      "100%|██████████| 43/43 [00:04<00:00, 10.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.1256 | Train Acc 0.9673 | Test Loss 0.2800 | Test Acc 0.9445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "max_norm = 0.25\n",
    "train_loss = 0\n",
    "train_acc = 0\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        x, y = batch\n",
    "\n",
    "        inputs = x.transpose(1, 0).contiguous().to(device)\n",
    "        targets = y.to(device)\n",
    "        clf_mask = (inputs == tokenizer.vocab['[CLS]']).to(device)\n",
    "        pad_mask = (x == tokenizer.vocab['[PAD]']).to(device)\n",
    "\n",
    "        logits = model(inputs, clf_tokens_mask=clf_mask, padding_mask=pad_mask)\n",
    "        loss = criterion(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += torch.sum(torch.argmax(logits, dim=1) == targets).item() / len(targets)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            x, y = batch\n",
    "\n",
    "            inputs = x.transpose(1, 0).contiguous().to(device)\n",
    "            targets = y.to(device)\n",
    "            clf_mask = (inputs == tokenizer.vocab['[CLS]']).to(device)\n",
    "            pad_mask = (x == tokenizer.vocab['[PAD]']).to(device)\n",
    "\n",
    "            logits = model(inputs, clf_tokens_mask=clf_mask, padding_mask=pad_mask)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_acc += torch.sum(torch.argmax(logits, dim=1) == targets).item() / len(targets)\n",
    "        test_loss /= len(test_loader)\n",
    "        test_acc /= len(test_loader)\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\"Train Loss {:.4f} | Train Acc {:.4f} | Test Loss {:.4f} | Test Acc {:.4f}\".format(train_loss, train_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we got a final test set accuracy of 94.45% in just three epochs of finetuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
